# -*- coding: utf-8 -*-
"""Exoplanet_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LmE4PS0X97zQY8s9XHb1F4cOTE9Cb5KY
"""

#libraries
import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_classif

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

"""# **Data Preprocessing**"""

file_path = '/content/drive/MyDrive/Spartificial/Project/Exoplanets datasets/koi_final.csv'
#file_path = '/content/CLEANED koi.csv'
df = pd.read_csv(file_path)

df.info()

df['koi_disposition'].value_counts()

from sklearn.impute import KNNImputer

KImputer = KNNImputer(n_neighbors=5)

koi_new = KImputer.fit_transform(df.iloc[:,1:])

koi_test = pd.DataFrame(koi_new,columns=df.columns[1:])

df_cleaned = pd.concat([df.iloc[:,0],koi_test],axis=1)

df_cleaned.head()

df_test = df_cleaned.copy()

scaler = StandardScaler()
df_test.iloc[:,1:] = scaler.fit_transform(df_test.iloc[:,1:])

#ANOVA F test: this test used variance of the two groups to check which parameter has high probability to seperae out two groups
confirmed_group = df_test[df_test.iloc[:, 0] == 'CONFIRMED']
false_positives_group = df_test[df_test.iloc[:, 0] == 'FALSE POSITIVE']
anova_results = []
for col in df_test.columns[1:]:
    f_val, p_val = stats.f_oneway(confirmed_group[col], false_positives_group[col])
    anova_results.append((col, f_val, p_val))

anova_results.sort(key=lambda x: x[1], reverse=True)
print("Column\t\tF-value\t\tp-value")
for result in anova_results:
    print(f"{result[0]}\t\t{result[1]:.4f}\t\t{result[2]:.4f}")

df_cleaned.drop(['koi_prad','koi_insol'],axis=1,inplace=True)

X = df_cleaned.drop('koi_disposition', axis=1)
y = df_cleaned['koi_disposition']

mi_scores = mutual_info_classif(X , y , discrete_features = False )

# Create a DataFrame to display the results
mi_results = pd.DataFrame({'Feature': X. columns , 'Mutual Information': mi_scores})
mi_results.sort_values(by ='Mutual Information', ascending = False ,inplace = True )
print(mi_results)

# we applied spearman correlation method to find how two parameters are interdependent on each other
numeric_data = df_cleaned.select_dtypes(include=[float, int])
corr = numeric_data.corr(method='spearman')
corr.style.background_gradient(cmap='coolwarm').format(precision=2)

sns.boxplot(data=df_cleaned,x='koi_disposition',y='koi_teq')

"""The above boxplot shows the distribution of surface temperature of confirmed exo planets and false positives. The maxinum temperature of confirmed planets are within the 2000K , with few outliers, but the max temperature of false positives is almost 4000K  with many outliers reaching temperature above 14000K.

 75% of the confirmed planets are within the 1000K range while false positives are reaching 2000k.
"""

sns.histplot(data=df_cleaned,x='koi_depth',hue='koi_disposition',bins=50)

"""The above histogram shows the brightness(koi_depth) of the confirmed and false positives. Most confirmed exoplanet and false positive's brightness is within 0.2 while few false positives being outliers reaching upto 0.9 in brightness.

Thus brightness is a weak indicator to tell whether the moving body is a exoplanet or false positive.

The above scatterplot between two features koi_insol and koi_teq shows high correlation between eachother. These features must be handled appropriately before training linear models to prevent unstable resulting model coefficients, giving misleading inferences
"""

# these columns have the highest ANOVA F score
subset_data = df_cleaned.iloc[:1000]
sns.scatterplot(data=subset_data,x='koi_teq',y='koi_depth',hue='koi_disposition',style='koi_disposition')

"""These two features show no correlation wiht each other and have a high F score in Anova test ,  which is perfect for machine learning models."""

sns.boxplot(data=df_cleaned,x='koi_disposition',y='koi_teq')

fig,axes = plt.subplots(3,4,figsize=(20,15))
cols = df_cleaned.columns[1:]

for col,ax in zip(cols,axes.flatten()):
  sns.boxplot(data=df_cleaned,x='koi_disposition',y=col,ax=ax)

plt.tight_layout()
plt.show()

X = df_cleaned.iloc[:,1:]
Y = df_cleaned.iloc[:,0]
print(X)
Y

func = lambda x: 1 if x == 'CONFIRMED' else 0
Y_encoded = Y.apply(func)
Y_encoded

X_train,X_test,Y_train,Y_test = train_test_split(X,Y_encoded,test_size=0.25,random_state=7,stratify=Y)

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train = pd.DataFrame(X_train,columns=X.columns)
X_test = pd.DataFrame(X_test,columns=X.columns)
Y_train = pd.DataFrame(Y_train,columns=['koi_disposition'])
Y_test = pd.DataFrame(Y_test,columns=['koi_disposition'])

X_train.to_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/X_train.csv',index=False)
X_test.to_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/X_test.csv',index=False)
Y_train.to_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/Y_train.csv',index=False)
Y_test.to_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/Y_test.csv',index=False)

X_train,X_test,Y_train,Y_test = train_test_split(X,Y_encoded,test_size=0.25,random_state=7,stratify=Y)

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

pca = PCA(n_components = 7)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

print(pca.explained_variance_ratio_)
print(sum(pca.explained_variance_ratio_))

plt.bar(list(range(1,len(pca.explained_variance_)+1)),pca.explained_variance_,alpha=0.5,align='center')
plt.ylabel("Variation explained")
plt.xlabel('eigen value')
plt.show()

X_train_pca_df = pd.DataFrame(X_train_pca,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7'])
X_test_pca_df = pd.DataFrame(X_test_pca,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7'])

X_train_pca_df.to_csv("/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/X_train_pca.csv",index=False)
X_test_pca_df.to_csv("/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/X_test_pca.csv",index=False)
Y_train.to_csv("/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/Y_train.csv",index=False)
Y_test.to_csv("/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/Y_test.csv",index=False)

"""# **Model Training**"""

from tensorflow.keras import losses
from tensorflow.keras import optimizers
from tensorflow.keras import Sequential
from tensorflow.keras.layers import InputLayer , Dense, BatchNormalization, Dropout
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

X_train = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/X_train.csv')
X_test = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/X_test.csv')
Y_train = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/Y_train.csv')
Y_test = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/Y_test.csv')

X_train_pca = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/X_train_pca.csv')
X_test_pca = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/X_test_pca.csv')
Y_train_pca = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/Y_train.csv')
Y_test_pca = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/With_PCA/Y_test.csv')

Lr = LogisticRegression(max_iter=1000)
Lr_pca = LogisticRegression(max_iter=1000)

Y_train = Y_train.values.ravel()
Y_test = Y_test.values.ravel()
Y_train_pca = Y_train_pca.values.ravel()
Y_test_pca = Y_test_pca.values.ravel()

Lr.fit(X_train,Y_train)
Lr_pca.fit(X_train_pca,Y_train_pca)

print(Lr.score(X_test,Y_test))
print(Lr_pca.score(X_test_pca,Y_test_pca))

Y_pred = Lr.predict(X_test)
Y_pred_pca = Lr_pca.predict(X_test_pca)

print("Without PCA:\n",classification_report(Y_test,Y_pred))

print("With PCA:\n",classification_report(Y_test_pca,Y_pred_pca))

Lr = LogisticRegression(max_iter=1000)

Lr.fit(X_train,Y_train)



X_train.shape[1]

model = Sequential()
model.add(InputLayer(shape=(X_train.shape[1],)))
model.add(Dense(units=750,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=500,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=250,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=100,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=50,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))

model.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

history = model.fit(X_train,Y_train,epochs=50,batch_size=32,validation_split=0.2)

model.evaluate(X_test,Y_test)

Y_predict = model.predict(X_test)
Y_predict = np.where(Y_predict>0.5,1,0)

Y_predict

print("Without PCA:\n",classification_report(Y_test,Y_predict))

model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)

model.save_weights("model.weights.h5")

model_pca = Sequential()
model_pca.add(InputLayer(shape=(X_train_pca.shape[1],)))
model_pca.add(Dense(units=750,activation='relu'))
model_pca.add(BatchNormalization())
model_pca.add(Dropout(0.3))
model_pca.add(Dense(units=500,activation='relu'))
model_pca.add(BatchNormalization())
model_pca.add(Dropout(0.3))
model_pca.add(Dense(units=250,activation='relu'))
model_pca.add(BatchNormalization())
model_pca.add(Dropout(0.3))
model_pca.add(Dense(units=100,activation='relu'))
model_pca.add(BatchNormalization())
model_pca.add(Dropout(0.3))
model_pca.add(Dense(units=50,activation='relu'))
model_pca.add(Dense(units=1,activation='sigmoid'))

model_pca.summary()

model_pca.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model_pca.fit(X_train_pca,Y_train_pca,epochs=50,batch_size=32,validation_split=0.2)

model_pca.evaluate(X_test_pca,Y_test_pca)

Y_predict_pca = model_pca.predict(X_test_pca)
Y_predict_pca = np.where(Y_predict_pca>0.5,1,0)

print("With PCA:\n",classification_report(Y_test_pca,Y_predict_pca))

model_pca_json = model_pca.to_json()
with open("model_pca.json", "w") as json_file:
    json_file.write(model_pca_json)

model_pca.save_weights("model_pca.weights.h5")

"""**Cross Validation**"""

from sklearn.model_selection import StratifiedKFold

cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=7)

fold_no = 1
acc_per_fold = []

for train, test in cv.split(X_train,Y_train):
  print(train)
  print(test)
  break

for train, test in cv.split(X_train,Y_train):
  train_X = X_train.iloc[train]
  train_Y = Y_train.iloc[train]
  test_X = X_train.iloc[test]
  test_Y = Y_train.iloc[test]


  model = Sequential()
  model.add(InputLayer(shape=(train_X.shape[1],)))
  model.add(Dense(units=750,activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.3))
  model.add(Dense(units=500,activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.3))
  model.add(Dense(units=250,activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.3))
  model.add(Dense(units=100,activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.3))
  model.add(Dense(units=50,activation='relu'))
  model.add(Dense(units=1,activation='sigmoid'))

  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
  history = model.fit(X_train,Y_train,epochs=50,batch_size=32,verbose=1)

  scores = model.evaluate(test_X,test_Y,verbose=0)
  acc_per_fold.append(scores[1]*100)

  fold_no += 1

acc_per_fold

model = Sequential()
model.add(InputLayer(shape=(X_train.shape[1],)))
model.add(Dense(units=750,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=500,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=250,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=100,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=50,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

history = model.fit(X_train,Y_train,epochs=50,batch_size=32,validation_split=0.2)

Y_predict = model.predict(X_test)
Y_predict = np.where(Y_predict>0.5,1,0)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test,Y_predict)
sns.heatmap(cm,fmt='d',annot=True)

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=7)

X_train_ros,Y_train_ros = ros.fit_resample(X_train,Y_train)

Y_train_ros.value_counts()

model = Sequential()
model.add(InputLayer(shape=(X_train.shape[1],)))
model.add(Dense(units=750,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=500,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=250,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=100,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=50,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

history = model.fit(X_train_ros,Y_train_ros,epochs=50,batch_size=32,validation_split=0.2)

Y_predict = model.predict(X_test)
Y_predict = np.where(Y_predict>0.5,1,0)

cm = confusion_matrix(Y_test,Y_predict)
sns.heatmap(cm,fmt='d',annot=True)

print(classification_report(Y_test,Y_predict))

"""# **Pipeline**"""

from tensorflow.keras import losses
from tensorflow.keras import optimizers
from tensorflow.keras import Sequential
from tensorflow.keras.layers import InputLayer , Dense, BatchNormalization, Dropout
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

X_train = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/X_train.csv')
X_test = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/X_test.csv')
Y_train = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/Y_train.csv')
Y_test = pd.read_csv('/content/drive/MyDrive/Spartificial/ExoPlanet_New_Datasets/without_PCA/Y_test.csv')

from imblearn.under_sampling import ClusterCentroids as CC
from imblearn.over_sampling import ADASYN
from imblearn.pipeline import Pipeline

model = Sequential()
model.add(InputLayer(shape=(X_train.shape[1],)))
model.add(Dense(units=750,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=500,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=250,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=100,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(units=50,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

#from sklearn.base import BaseEstimator, TransformerMixin

#class CustomEsitmator(BaseEstimator,TransformerMixin):
 # def __init__(self,nn_model,classification_report):
   # self.nn_model = nn_model
    # self.classification_report = classification_report

  # def fit(self,X,Y):
   # return self

  #def transform(self,X,Y=None):
   # Y_predict = self.nn_model(X,Y)
   # self.classification_report(Y,Y_predict)

pipeline = Pipeline(steps=[('cc',CC(random_state=7,sampling_strategy =0.7)),('adasyn',ADASYN(random_state=7,sampling_strategy =0.9))])

X_train_final,Y_train_final = pipeline.fit_resample(X_train,Y_train)

Y_train_final.value_counts()

Y_train.value_counts()

history = model.fit(X_train_final,Y_train_final,epochs=50,batch_size=32,validation_split=0.2)

Y_predict = model.predict(X_test)
Y_predict = np.where(Y_predict>0.5,1,0)

cm = confusion_matrix(Y_test,Y_predict)
sns.heatmap(cm,fmt='d',annot=True)

print(classification_report(Y_test,Y_predict))

